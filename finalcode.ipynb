{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809f217-d86e-4436-806f-beb7a3dd436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers neo4j matplotlib torch-geometric spacy networkx\n",
    "!pip install stanza torchtext torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from neo4j import GraphDatabase\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set environment variables with the correct Neo4j credentials using Python\n",
    "os.environ['NEO4J_URI'] = 'neo4j+s://1cc6fff5.databases.neo4j.io'\n",
    "os.environ['NEO4J_USER'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = 'meX8mx5oh5L8uJvK8XYBtdO68aFJoDYtlTvhkMZWx-k'\n",
    "\n",
    "# Define a custom dataset class for BERT\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Neo4j connection details\n",
    "def connect_to_neo4j(uri, user, password):\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        print(\"Connected to Neo4j successfully.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "\n",
    "# Updated graph creation function with meaningful node names and properties\n",
    "def create_knowledge_graph(driver, entities, relationships):\n",
    "    if driver is None:\n",
    "        print(\"Cannot create graph, Neo4j driver not connected.\")\n",
    "        return\n",
    "\n",
    "    with driver.session() as session:\n",
    "        try:\n",
    "            # Use MERGE to create unique nodes with specific attributes\n",
    "            for entity in entities:\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MERGE (n:FinancialEntity {name: $name})\n",
    "                    ON CREATE SET n.type = $type, n.year = $year, n.created = timestamp()\n",
    "                    \"\"\",\n",
    "                    name=entity['name'], type=entity.get('type', 'Unknown'), year=entity.get('year', 'Unknown')\n",
    "                )\n",
    "\n",
    "            # Use descriptive relationship types with meaningful properties\n",
    "            for rel in relationships:\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (a:FinancialEntity {name: $from_name}), (b:FinancialEntity {name: $to_name})\n",
    "                    MERGE (a)-[r:CAUSES {type: $rel_type, created: timestamp()}]->(b)\n",
    "                    \"\"\",\n",
    "                    from_name=rel[0], to_name=rel[1], rel_type=rel[2]\n",
    "                )\n",
    "            print(\"Knowledge graph created successfully with named nodes and relationships.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the knowledge graph: {e}\")\n",
    "\n",
    "# Load data from nested directory structure\n",
    "def load_data_from_directory(base_dir):\n",
    "    data = []\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Path does not exist: {base_dir}\")\n",
    "        return data\n",
    "\n",
    "    for subdir, _, files in os.walk(base_dir):\n",
    "        for filename in files:\n",
    "            if filename == 'full-submission.txt':\n",
    "                file_path = os.path.join(subdir, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    text = file.read()\n",
    "                    if text.strip():  # Ensure non-empty content\n",
    "                        data.append({'text': text, 'label': 0})  # Replace 'label' logic as needed\n",
    "                    else:\n",
    "                        print(f\"Empty content in file: {filename} at {subdir}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No valid data loaded from the directory.\")\n",
    "    return data\n",
    "\n",
    "# Combine data from multiple years\n",
    "def combine_data_from_years(base_dir, selected_years):\n",
    "    combined_data = []\n",
    "    for year in selected_years:\n",
    "        matching_folders = [f for f in os.listdir(os.path.join(base_dir, '10-K')) if f'-{year[-2:]}-' in f]\n",
    "        if not matching_folders:\n",
    "            print(f\"No folder found for the year {year}.\")\n",
    "            continue\n",
    "        year_folder = matching_folders[0]\n",
    "        year_dir = os.path.join(base_dir, '10-K', year_folder)\n",
    "        data = load_data_from_directory(year_dir)\n",
    "        combined_data.extend(data)\n",
    "    return combined_data\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Example: tokenization, cleaning, etc.\n",
    "    # Return processed data\n",
    "    return data\n",
    "\n",
    "def plot_metrics(training_losses, validation_losses):\n",
    "    \"\"\"\n",
    "    Plot the training and validation losses over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_bert(data, base_dir, epochs=3, batch_size=8, learning_rate=5e-5):\n",
    "    # Initialize TensorBoard\n",
    "    writer = SummaryWriter(log_dir=os.path.join(base_dir, 'runs/bert_experiment'))\n",
    "\n",
    "    # Check if there's enough data\n",
    "    if len(data) < 2:\n",
    "        print(f\"Not enough data to perform train-test split. Found {len(data)} sample(s).\")\n",
    "        return\n",
    "\n",
    "    # Example preprocessing call\n",
    "    processed_data = preprocess_data(data)\n",
    "    sentences = [item['text'] for item in processed_data]\n",
    "    labels = [item['label'] for item in processed_data]\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    try:\n",
    "        train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "            sentences, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in train-test split: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize data\n",
    "    train_encodings = tokenizer(train_sentences, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    val_encodings = tokenizer(val_sentences, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = BertDataset(train_encodings, train_labels)\n",
    "    val_dataset = BertDataset(val_encodings, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Check if CUDA is available and set up mixed precision training accordingly\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    scaler = GradScaler(enabled=use_cuda)  # Enable scaler only if CUDA is available\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Training without CUDA\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "\n",
    "        # Log training loss to TensorBoard\n",
    "        writer.add_scalar('Training Loss', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        validation_losses.append(avg_val_loss)\n",
    "\n",
    "        # Log validation loss to TensorBoard\n",
    "        writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = os.path.join(base_dir, \"bert_model\")\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Model and tokenizer saved to {model_save_path}\")\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plot_metrics(training_losses, validation_losses)\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "def get_available_years(base_dir):\n",
    "    \"\"\"\n",
    "    List available years based on folder naming conventions for the selected company.\n",
    "    \"\"\"\n",
    "    ten_k_path = os.path.join(base_dir, '10-K')\n",
    "    if not os.path.exists(ten_k_path):\n",
    "        print(f\"Path does not exist: {ten_k_path}\")\n",
    "        return []\n",
    "\n",
    "    years = set()\n",
    "    for folder_name in os.listdir(ten_k_path):\n",
    "        match = re.search(r'-(\\d{2})-', folder_name)  # Extract the year part like '23' in '0001652044-23-000016'\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if year <= 23:  # Assumes files from 2000 onwards\n",
    "                years.add(f\"20{year:02d}\")\n",
    "            else:\n",
    "                years.add(f\"19{year:02d}\")\n",
    "\n",
    "    return sorted(years)\n",
    "\n",
    "def main():\n",
    "    # Prompt user for company selection\n",
    "    companies = ['AAPL', 'AMZN', 'GOOGL', 'MSFT']\n",
    "    print(f\"Available companies: {companies}\")\n",
    "    company = input(\"Select the company for analysis: \").strip().upper()\n",
    "    if company not in companies:\n",
    "        print(\"Invalid company selected.\")\n",
    "        return\n",
    "\n",
    "    base_dir = f'/content/drive/MyDrive/datasets/sec-edgar-filings/{company}'\n",
    "    print(f\"Searching for files in: {base_dir}\")\n",
    "\n",
    "    # Get available years for the selected company\n",
    "    available_years = get_available_years(base_dir)\n",
    "    print(f\"Available years for {company}: {available_years}\")\n",
    "    if not available_years:\n",
    "        print(\"No available years found.\")\n",
    "        return\n",
    "\n",
    "    # Prompt user for multiple year selection\n",
    "    selected_years = input(\"Select the years for analysis (comma separated, e.g., 2016,2017): \").strip().split(',')\n",
    "    combined_data = combine_data_from_years(base_dir, selected_years)\n",
    "\n",
    "    if not combined_data:\n",
    "        print(\"No data to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Train BERT model using combined data\n",
    "    train_bert(combined_data, base_dir, epochs=30)\n",
    "\n",
    "    # Connect to Neo4j and create the knowledge graph\n",
    "    uri = os.getenv(\"NEO4J_URI\")\n",
    "    user = os.getenv(\"NEO4J_USER\")\n",
    "    password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "    driver = connect_to_neo4j(uri, user, password)\n",
    "\n",
    "    # Example entities and relationships for creating a causality knowledge graph\n",
    "    entities = [\n",
    "        {\"name\": \"Revenue\", \"type\": \"Metric\", \"year\": \"2023\"},\n",
    "        {\"name\": \"Profit\", \"type\": \"Metric\", \"year\": \"2023\"},\n",
    "        {\"name\": \"Expenses\", \"type\": \"Metric\", \"year\": \"2023\"},\n",
    "        {\"name\": \"Market Growth\", \"type\": \"External Factor\", \"year\": \"2023\"}\n",
    "    ]\n",
    "\n",
    "    relationships = [\n",
    "        (\"Revenue\", \"Profit\", \"CAUSES\"),\n",
    "        (\"Expenses\", \"Profit\", \"DECREASES\"),\n",
    "        (\"Market Growth\", \"Revenue\", \"INCREASES\")\n",
    "    ]\n",
    "\n",
    "    create_knowledge_graph(driver, entities, relationships)\n",
    "\n",
    " # New print statements added here\n",
    "    print(\"\\nProcessing complete. You can now visualize the graph in Neo4j Browser.\")\n",
    "    print(\"Sample Cypher queries:\")\n",
    "    print(\"\\n1. For Market Growth as a Cause use:\\n\\n MATCH (n:FinancialEntity {name: 'Market Growth'})-[r:CAUSES]->(m:FinancialEntity) RETURN n, r, m;\\n\")\n",
    "    print(\"\\n2. Causal Relationships Among Core Financial Metrics use:\\n\\n MATCH (n:FinancialEntity)-[r:CAUSES]->(m:FinancialEntity) WHERE n.name IN ['Revenue', 'Profit', 'Expenses', 'Growth', 'Loss'] AND m.name IN ['Revenue', 'Profit', 'Expenses', 'Growth', 'Loss'] RETURN n, r, m;\")\n",
    "    print(\"\\n3. For Revenue Relationships use: \\n \\nMATCH (n:FinancialEntity {name: 'Revenue'})-[r]->(m:FinancialEntity) RETURN n, r, m;\")\n",
    "    print(\"\\n4. To view all relationships: \\n\\nMATCH (n) RETURN n LIMIT 25\\n\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
